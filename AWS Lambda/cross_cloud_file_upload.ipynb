{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c26e9-bc94-4d87-96b3-7f558bebf28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import gzip\n",
    "import io\n",
    "import logging\n",
    "from datetime import datetime, time, timedelta\n",
    "import pytz\n",
    "from botocore.exceptions import ClientError\n",
    "from google.cloud import storage\n",
    "import urllib.parse\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def get_secret():\n",
    "    \"\"\"Retrieve the GCS Auth Key stored in AWS secretsmanager\"\"\"\n",
    "    \n",
    "    secret_name = \"aws-gcs-ascent-write-access\"\n",
    "    region_name = \"ap-south-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        print(f\"Error retrieving secret: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Secrets are typically stored as JSON strings, so we need to parse it\n",
    "    secret = get_secret_value_response.get('SecretString')\n",
    "\n",
    "    if secret:\n",
    "        secret_dict = json.loads(secret)\n",
    "\n",
    "        # Extract the specific key-value pairs from the secret\n",
    "        access_key = secret_dict.get(\"json_key\")\n",
    "        return access_key\n",
    "\n",
    "    else:\n",
    "        print(\"SecretString is empty or not in a valid format.\")\n",
    "        return None\n",
    "\n",
    "def assume_role(role_arn):\n",
    "    \"\"\"Use STS Client to Assume aws cross account role\"\"\"\n",
    "\n",
    "    # Initialize the STS client to assume role\n",
    "    sts_client = boto3.client('sts')\n",
    "\n",
    "    assumed_role_object = sts_client.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=\"LambdaS3AccessSession\"\n",
    "    )\n",
    "    credentials = assumed_role_object['Credentials']\n",
    "\n",
    "    # Return the temporary credentials\n",
    "    return credentials\n",
    "\n",
    "\n",
    "def compress_file(file_content):\n",
    "    # Compress the file content using gzip, for egress data transfer cost saving\n",
    "    compressed_data = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=compressed_data, mode='wb') as gz:\n",
    "        gz.write(file_content)\n",
    "    compressed_data.seek(0)\n",
    "    return compressed_data\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    \"\"\"Lambda function intend to upload file on GCS on each file upload on s3\n",
    "        This setup is based on S3<>Lambda Trigger, with Secretsmanager used to\n",
    "        store the GCS Authentication secrets\"\"\"\n",
    "    \n",
    "    # Capturing the event details\n",
    "    event_bucket = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n",
    "    event_object_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n",
    "    decoded_event_object_key = urllib.parse.unquote(event_object_key)\n",
    "    event_object_key = decoded_event_object_key\n",
    "\n",
    "    # Parse the event_key, file_name and table_name\n",
    "    event_key = event_object_key.split('/')[-2:]\n",
    "    file_name = event_object_key.split('/')[-1]\n",
    "    table_name = event_object_key.split('/')[-4]\n",
    "\n",
    "    logging.info(f'Event Bucket: {event_bucket}')\n",
    "    logging.info(f'Event Object Key: {event_object_key}')\n",
    "    logging.info(f'Event Key: {event_key}')\n",
    "    logging.info(f'Event file Name: {file_name}')\n",
    "    logging.info(f'Event table Name: {table_name}')\n",
    "\n",
    "    # Role ARN from the S3 account\n",
    "    role_arn = 'arn:aws:iam::<AWS_Account_Id>:role/as-s3-role-ahwspl-skull-etl-rw'\n",
    "\n",
    "    # Assume the role to get temporary credentials for cross AWS account access \n",
    "    # Required for AWS cross account access s3<>Lambda trigger\n",
    "    \n",
    "    credentials = assume_role(role_arn)\n",
    "\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Processing file: {file_name}\")\n",
    "\n",
    "        # Get the file content and file size\n",
    "        file_obj = s3_client.get_object(Bucket=event_bucket, Key=event_object_key)\n",
    "\n",
    "        file_content = file_obj['Body'].read()\n",
    "        original_size = len(file_content)\n",
    "        logging.info(f\"Original file size: {original_size} bytes\")\n",
    "        \n",
    "        # Compress the file content and file size\n",
    "        compressed_content = compress_file(file_content)\n",
    "        compressed_size = compressed_content.getbuffer().nbytes\n",
    "        logging.info(f\"Compressed file size: {compressed_size} bytes\")\n",
    "\n",
    "        # Define target path\n",
    "        # Get the current time in GMT\n",
    "        utc_time = datetime.now(pytz.utc)\n",
    "\n",
    "        # Convert GMT to IST\n",
    "        ist = pytz.timezone('Asia/Kolkata')\n",
    "        ist_time = utc_time.astimezone(ist)\n",
    "\n",
    "        # Print both GMT and IST times in YYYY-HH-MM format\n",
    "        logging.info(f\"UTC Time: {utc_time.strftime('%Y-%m-%d %H:%M %Z%z')}\")\n",
    "        logging.info(f\"IST Time: {ist_time.strftime('%Y-%m-%d %H:%M %Z%z')}\")\n",
    "\n",
    "        target_path = 'prod/dataplatform_raw_payload/' + table_name + '/dt=' + ist_time.strftime(\n",
    "            '%Y-%m-%d') + '/' + file_name\n",
    "\n",
    "        # Upload the compressed file to the target S3 path\n",
    "        s3_client.upload_fileobj(\n",
    "            compressed_content,\n",
    "            bucket_name,\n",
    "            target_path,\n",
    "            ExtraArgs={'ContentType': 'application/gzip'}\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Compressed file uploaded to: {target_path}\")\n",
    "\n",
    "        # lambda to GCS Access\n",
    "        try:\n",
    "            access_key = get_secret()\n",
    "\n",
    "            # Parse the access_key string into a dictionary\n",
    "            access_key_dict = json.loads(access_key)\n",
    "\n",
    "            # Initialize Google Cloud Storage Client\n",
    "            client = storage.Client.from_service_account_info(access_key_dict)\n",
    "\n",
    "            # Access your bucket\n",
    "            bucket = client.get_bucket(event_bucket)\n",
    "\n",
    "            # Create a blob object from the target path\n",
    "            blob = bucket.blob(target_path)\n",
    "\n",
    "            # Upload the compressed content\n",
    "            blob.upload_from_file(compressed_content, content_type='application/gzip')\n",
    "\n",
    "            logging.info(f\"Compressed file uploaded to: gs://{event_bucket}/{target_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('Files compressed and uploaded successfully.')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error accessing S3: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps(f\"Error accessing S3: {str(e)}\")\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
